{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from decimal import Decimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "HUMAN_MOTIF_VARIANTS = [\n",
    "    'AATAAA',\n",
    "    'ATTAAA',\n",
    "    'AAAAAG',\n",
    "    'AAGAAA',\n",
    "    'TATAAA',\n",
    "    'AATACA',\n",
    "    'AGTAAA',\n",
    "    'ACTAAA',\n",
    "    'GATAAA',\n",
    "    'CATAAA',\n",
    "    'AATATA',\n",
    "    'AATAGA'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_PATH = 'data/human/dragon_polyA_data/positive5fold/'\n",
    "NEG_PATH = 'data/human/dragon_polyA_data/negatives5fold/'\n",
    "# POS_PATH = 'human_data/omni_polyA_data/positive/'\n",
    "# NEG_PATH = 'human_data/omni_polyA_data/negative/'\n",
    "BATCH_SIZE = 64\n",
    "PATCH_SIZE = 10\n",
    "DEPTH = 16\n",
    "NUM_HIDDEN = 64\n",
    "SEQ_LEN = 206 + 2*PATCH_SIZE-2\n",
    "NUM_CHANNELS = 4\n",
    "NUM_LABELS = 2\n",
    "NUM_EPOCHS = 200\n",
    "NUM_FOLDS = 5\n",
    "HYPER_DICT = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#用户可以选择输入的目录等参数\n",
    "tf.app.flags.DEFINE_string('train_dir', None, 'Directory where checkpoints are written to.') \n",
    "tf.app.flags.DEFINE_integer('training_job_index', 0, 'index of training result for logging')\n",
    "tf.app.flags.DEFINE_string('training_result_dir', None, 'The file which the training result is written to')\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#这一块其实用不到，不用管\n",
    "# Disable print\n",
    "def block_print():\n",
    "    sys.stdout = open(os.devnull, 'w')\n",
    "\n",
    "\n",
    "# Restore print\n",
    "def enable_print():\n",
    "    sys.stdout = sys.__stdout__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#从文件路径取到文件\n",
    "def get_motif_data(data_root, label):\n",
    "    data = {}\n",
    "    labels = {}\n",
    "    for motif in HUMAN_MOTIF_VARIANTS:   #把数据按照motif分类放好\n",
    "        data[motif] = []\n",
    "        for data_file in os.listdir(data_root):\n",
    "            if motif in data_file:\n",
    "                data_path = os.path.join(data_root, data_file)\n",
    "                with open(data_path, 'r') as f:\n",
    "                    alphabet = np.array(['A','G','T','C'])\n",
    "                    for line in f:\n",
    "                        line = list(line.strip('\\n'))\n",
    "                        seq = np.array(line, dtype = '|U1').reshape(-1,1)\n",
    "                        seq_data = (seq == alphabet).astype(np.float32) \n",
    "                        data[motif].append(seq_data)                       #转化成one-hot表示法 如1 1 0 1\n",
    "        data[motif] = np.stack(data[motif]).reshape([-1,206,1,4])\n",
    "        if label:\n",
    "            labels[motif] = np.zeros(data[motif].shape[0])      \n",
    "        else:\n",
    "            labels[motif] = np.ones(data[motif].shape[0])\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(dataset, labels, randomState = None):     #把数据随机打乱\n",
    "    if randomState is None:\n",
    "        permutation = np.random.permutation(labels.shape[0])\n",
    "    else:\n",
    "        permutation = randomState.permutation(labels.shape[0])\n",
    "    shuffled_data = dataset[permutation,:,:]\n",
    "    shuffled_labels = labels[permutation]\n",
    "    return shuffled_data, shuffled_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_motif_dataset(num_folds, pos_path, neg_path, seed=0):     #调用上面两个函数，放好数据\n",
    "    pos_data, pos_labels = get_motif_data(pos_path, True)\n",
    "    neg_data, neg_labels = get_motif_data(neg_path, False)\n",
    "    randomState = np.random.RandomState(seed)\n",
    "    for motif in HUMAN_MOTIF_VARIANTS:\n",
    "        pos_data[motif],pos_labels[motif] = shuffle(pos_data[motif], pos_labels[motif], randomState)\n",
    "        neg_data[motif],neg_labels[motif] = shuffle(neg_data[motif], neg_labels[motif], randomState)\n",
    "        print('Positive %s:'%motif, pos_data[motif].shape, pos_labels[motif].shape)\n",
    "        print('Negative %s:'%motif, neg_data[motif].shape, neg_labels[motif].shape)\n",
    "    return pos_data, pos_labels, neg_data, neg_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_hyper_dict(hyper_dict=None):   #随机设置超参数\n",
    "    def rand_log(a,b):\n",
    "        x = np.random.sample()\n",
    "        return 10.0 ** ((np.log10(b) - np.log10(a)) * x + np.log10(a))\n",
    "    \n",
    "    def rand_sqrt(a,b):\n",
    "        x = np.random.sample()\n",
    "        return (b - a) * np.sqrt(x) + a\n",
    "    \n",
    "    if hyper_dict is None:\n",
    "        hyper_dict = {\n",
    "            'tf_learning_rate': rand_log(.0005, .05),\n",
    "            'tf_momentum': rand_sqrt(.95,.99),\n",
    "            'tf_motif_init_weight': rand_log(1e-2,10),\n",
    "            'tf_fc_init_weight': rand_log(1e-2,10),\n",
    "            'tf_motif_weight_decay': rand_log(1e-5, 1e-3),\n",
    "            'tf_fc_weight_decay': rand_log(1e-5, 1e-3),\n",
    "            'tf_keep_prob': np.random.choice([.5, .75, 1.0]),\n",
    "            'tf_ngroups': np.random.choice([2,4,8])\n",
    "        }\n",
    "    #for k, v in hyper_dict.items():\n",
    "    #    print(\"%s: %.2e\"%(k, Decimal(v)))\n",
    "    #    print()\n",
    "    return hyper_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将数据分成五份，用于交叉验证，并设置train，valid，test数据集\n",
    "def data_split(pos_data, pos_labels, neg_data, neg_labels, num_folds, split):\n",
    "    pos_data_folds = np.array_split(pos_data, num_folds)\n",
    "    neg_data_folds = np.array_split(neg_data, num_folds)\n",
    "    pos_label_folds = np.array_split(pos_labels, num_folds)\n",
    "    neg_label_folds = np.array_split(neg_labels, num_folds)\n",
    "    \n",
    "    train_pos_data = np.concatenate([pos_data_folds[i] for i in split['train']], axis = 0)\n",
    "    train_pos_labels = np.concatenate([pos_label_folds[i] for i in split['train']], axis=0)\n",
    "    valid_pos_data = np.concatenate([pos_data_folds[i] for i in split['valid']], axis=0)\n",
    "    valid_pos_labels = np.concatenate([pos_label_folds[i] for i in split['valid']], axis=0)\n",
    "    \n",
    "    train_neg_data = np.concatenate([neg_data_folds[i] for i in split['train']], axis=0)\n",
    "    train_neg_labels = np.concatenate([neg_label_folds[i] for i in split['train']], axis=0)\n",
    "    valid_neg_data = np.concatenate([neg_data_folds[i] for i in split['valid']], axis=0)\n",
    "    valid_neg_labels = np.concatenate([neg_label_folds[i] for i in split['valid']], axis=0)\n",
    "    \n",
    "    train_data = np.concatenate((train_pos_data, train_neg_data), axis=0)\n",
    "    valid_data = np.concatenate((valid_pos_data, valid_neg_data), axis=0)\n",
    "    train_labels = np.concatenate((train_pos_labels, train_neg_labels), axis=0)\n",
    "    valid_labels = np.concatenate((valid_pos_labels, valid_neg_labels), axis=0)\n",
    "    \n",
    "    data = {}\n",
    "    data['train_dataset'], data['train_labels'] = shuffle(train_data, train_labels)\n",
    "    data['valid_dataset'], data['valid_labels'] = shuffle(valid_data, valid_labels)\n",
    "    \n",
    "    if 'test' in split:\n",
    "        test_pos_data = np.concatenate([pos_data_folds[i] for i in split['test']], axis=0)\n",
    "        test_pos_labels = np.concatenate([pos_label_folds[i] for i in split['test']], axis=0)\n",
    "        test_neg_data = np.concatenate([neg_data_folds[i] for i in split['test']], axis=0)\n",
    "        test_neg_labels = np.concatenate([neg_label_folds[i] for i in split['test']], axis=0)\n",
    "        test_data = np.concatenate((test_pos_data, test_neg_data), axis=0)\n",
    "        test_labels = np.concatenate((test_pos_labels, test_neg_labels), axis=0)\n",
    "        data['test_dataset'], data['test_labels'] = shuffle(test_data, test_labels)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将每一个motif的数据分开，设置train，valid，test数据集\n",
    "def motif_data_split(pos_data, pos_labels, neg_data, neg_labels, num_folds, split):\n",
    "    motif_data = {}\n",
    "    for motif in HUMAN_MOTIF_VARIANTS:\n",
    "        motif_data[motif] = data_split(pos_data[motif], pos_labels[motif], neg_data[motif], neg_labels[motif], num_folds, split)\n",
    "    \n",
    "    train_data = np.concatenate([motif_data[motif]['train_dataset'] for motif in HUMAN_MOTIF_VARIANTS], axis=0)\n",
    "    valid_data = np.concatenate([motif_data[motif]['valid_dataset'] for motif in HUMAN_MOTIF_VARIANTS], axis=0)\n",
    "    test_data = np.concatenate([motif_data[motif]['test_dataset'] for motif in HUMAN_MOTIF_VARIANTS], axis=0)\n",
    "    train_labels = np.concatenate([motif_data[motif]['train_labels'] for motif in HUMAN_MOTIF_VARIANTS], axis=0)\n",
    "    valid_labels = np.concatenate([motif_data[motif]['valid_labels'] for motif in HUMAN_MOTIF_VARIANTS], axis=0)\n",
    "    test_labels = np.concatenate([motif_data[motif]['test_labels'] for motif in HUMAN_MOTIF_VARIANTS], axis=0)\n",
    "    \n",
    "    data = {}\n",
    "    data['train_dataset'], data['train_labels'] = shuffle(train_data, train_labels)\n",
    "    data['valid_dataset'], data['valid_labels'] = shuffle(valid_data, valid_labels)\n",
    "    data['test_dataset'], data['test_labels'] = shuffle(test_data, test_labels)\n",
    "    \n",
    "    data['motif_dataset'] = {motif: {} for motif in HUMAN_MOTIF_VARIANTS}\n",
    "    for motif in HUMAN_MOTIF_VARIANTS:\n",
    "        data['motif_dataset'][motif]['test_dataset'] = motif_data[motif]['test_dataset']\n",
    "        data['motif_dataset'][motif]['test_labels'] = motif_data[motif]['test_labels']\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#填充数据用于卷积\n",
    "def pad_dataset(dataset, labels):\n",
    "    new_dataset = np.ones([dataset.shape[0], dataset.shape[1] + 2*PATCH_SIZE-2, dataset.shape[2], dataset.shape[3]], dtype = np.float32) * 0.25\n",
    "    new_dataset[:,PATCH_SIZE-1:-(PATCH_SIZE-1),:,:] = dataset\n",
    "    labels = (np.arange(NUM_LABELS) == labels[:,None]).astype(np.float32)\n",
    "    return new_dataset, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#计算预测精确度\n",
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels,1)) / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, hyper_dict):\n",
    "    graph = tf.Graph()\n",
    "    \n",
    "    with graph.as_default():\n",
    "        \n",
    "        #取到参数\n",
    "        tf_learning_rate = hyper_dict['tf_learning_rate']\n",
    "        tf_momentum = hyper_dict['tf_momentum']\n",
    "        tf_motif_init_weight = hyper_dict['tf_motif_init_weight']\n",
    "        tf_fc_init_weight = hyper_dict['tf_fc_init_weight']\n",
    "        tf_motif_weight_decay = hyper_dict['tf_motif_weight_decay']\n",
    "        tf_fc_weight_decay = hyper_dict['tf_fc_weight_decay']\n",
    "        tf_keep_prob = hyper_dict['tf_keep_prob']\n",
    "        tf_ngroups = hyper_dict['tf_ngroups']\n",
    "        \n",
    "        #原始数据\n",
    "        tf_train_dataset = tf.placeholder(tf.float32, shape = (BATCH_SIZE, SEQ_LEN, 1, NUM_CHANNELS))\n",
    "        tf_train_labels = tf.placeholder(tf.float32, shape = (BATCH_SIZE, NUM_LABELS))\n",
    "        tf_train_valid_dataset = tf.constant(dataset['train_dataset'])\n",
    "        tf_valid_dataset = tf.constant(dataset['valid_dataset'])\n",
    "        tf_test_dataset = tf.constant(dataset['test_dataset'])\n",
    "        tf_motif_test_dataset = {}\n",
    "        for motif in HUMAN_MOTIF_VARIANTS:\n",
    "            tf_motif_test_dataset[motif] = tf.constant(dataset['motif_dataset'][motif]['test_dataset'])\n",
    "        \n",
    "        #各层参数\n",
    "        conv_weights = tf.Variable(tf.truncated_normal([PATCH_SIZE, 1, NUM_CHANNELS, DEPTH], stddev=tf_motif_init_weight))\n",
    "        conv_biases = tf.Variable(tf.zeros([DEPTH]))\n",
    "        layer1_weights = tf.Variable(tf.truncated_normal([21*DEPTH, NUM_HIDDEN], stddev = tf_fc_init_weight))\n",
    "        layer1_biases = tf.Variable(tf.constant(1.0, shape=[NUM_HIDDEN]))\n",
    "        layer2_weights = tf.Variable(tf.truncated_normal([NUM_HIDDEN,NUM_LABELS], stddev=tf_fc_init_weight))\n",
    "        layer2_biases = tf.Variable(tf.constant(1.0, shape=[NUM_LABELS]))\n",
    "        \n",
    "        weights = {}\n",
    "        weights['conv_weights'] = conv_weights\n",
    "        weights['conv_biases'] = conv_biases\n",
    "        weights['layer1_weights'] = layer1_weights\n",
    "        weights['layer1_biases'] = layer1_biases\n",
    "        weights['layer2_weights'] = layer2_weights\n",
    "        weights['layer2_biases'] = layer2_biases\n",
    "        \n",
    "        #主要的模型\n",
    "        def model(data, drop=True):\n",
    "            conv = tf.nn.conv2d(data, conv_weights, [1,1,1,1], padding='VALID')\n",
    "            conv = tf.reshape(conv, [-1, 215, 1, DEPTH//tf_ngroups, tf_ngroups])\n",
    "            mu, var = tf.nn.moments(conv, [1,2,3], keep_dims=True)\n",
    "            conv = (conv - mu) / tf.sqrt(var + 1e-12)\n",
    "            conv = tf.reshape(conv, [-1, 215, 1, DEPTH])\n",
    "            hidden = tf.nn.relu(conv + conv_biases)\n",
    "            hidden = tf.nn.max_pool(hidden, [1,10,1,1], [1,10,1,1], padding = \"VALID\")\n",
    "            shape = hidden.get_shape().as_list()\n",
    "            motif_score = tf.reshape(hidden, [shape[0], shape[1]*DEPTH])\n",
    "            if drop:\n",
    "                hidden_nodes = tf.nn.dropout(tf.nn.relu(tf.matmul(motif_score, layer1_weights) + layer1_biases), tf_keep_prob)\n",
    "            else:\n",
    "                hidden_nodes = tf.nn.relu(tf.matmul(motif_score, layer1_weights) + layer1_biases)\n",
    "            return tf.matmul(hidden_nodes, layer2_weights) + layer2_biases\n",
    "        logits = model(tf_train_dataset)\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) + tf_fc_weight_decay * (tf.nn.l2_loss(layer1_weights) + tf.nn.l2_loss(layer2_weights)) + tf_motif_weight_decay*tf.nn.l2_loss(conv_weights)\n",
    "        \n",
    "        #optimizer， 实现梯度下降\n",
    "        global_step = tf.Variable(0, trainable=False)\n",
    "        stepOp = tf.assign_add(global_step, 1).op\n",
    "        learning_rate = tf.train.exponential_decay(tf_learning_rate, global_step, 3000, 0.96)\n",
    "        optimizer = tf.train.MomentumOptimizer(learning_rate, tf_momentum).minimize(loss)\n",
    "        \n",
    "        #predictions 预测结果\n",
    "        train_prediction = tf.nn.softmax(model(tf_train_valid_dataset, drop = False))\n",
    "        valid_prediction = tf.nn.softmax(model(tf_valid_dataset, drop = False))\n",
    "        test_prediction = tf.nn.softmax(model(tf_test_dataset, drop = False))\n",
    "        motif_test_prediction = {}\n",
    "        for motif in HUMAN_MOTIF_VARIANTS:\n",
    "            motif_test_prediction[motif] = tf.nn.softmax(model(tf_motif_test_dataset[motif], drop = False))\n",
    "            \n",
    "    train_resuts = []\n",
    "    valid_results = []\n",
    "    test_results = []\n",
    "    motif_test_results = {motif: [] for motif in HUMAN_MOTIF_VARIANTS}\n",
    "    save_weights = []\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "        train_dataset = dataset['train_dataset']\n",
    "        train_labels = dataset['train_labels']\n",
    "        np.random.seed()\n",
    "        print(\"Initialized\")\n",
    "        print('Training accuracy at begining: %.1f%%' % accuracy(train_prediction.eval(), train_labels))\n",
    "        print('Validation accuracy at the beginning: %.1f%%' % accuracy(valid_prediction.eval(), dataset['valid_labels']))\n",
    "        for epoch in range(NUM_EPOCHS):                         #对于每一个epoch\n",
    "            permutation = np.random.permutation(train_labels.shape[0])\n",
    "            shuffled_dataset = train_dataset[permutation,:,:]\n",
    "            shuffled_labels = train_labels[permutation,:]\n",
    "            for step in range(shuffled_labels.shape[0]//BATCH_SIZE):     #对于一个epoch里每一个batch的数据\n",
    "                offset = step * BATCH_SIZE\n",
    "                batch_data = train_dataset[offset:(offset + BATCH_SIZE),:,:,:]\n",
    "                batch_labels = train_labels[offset:(offset + BATCH_SIZE), :]\n",
    "                feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels}\n",
    "                _, l = session.run([optimizer,loss], feed_dict = feed_dict)      #调用optimizer，实现梯度下降\n",
    "                session.run(stepOp)\n",
    "            \n",
    "            train_resuts.append(accuracy(train_prediction.eval(), train_labels))    #计算结果并储存\n",
    "            valid_pred = valid_prediction.eval()\n",
    "            valid_results.append(accuracy(valid_pred, dataset['valid_labels']))\n",
    "            test_pred = test_prediction.eval()\n",
    "            test_results.append(accuracy(test_pred, dataset['test_labels']))\n",
    "            for motif in HUMAN_MOTIF_VARIANTS:\n",
    "                motif_test_pred = motif_test_prediction[motif].eval()\n",
    "                motif_test_results[motif].append(accuracy(motif_test_pred, dataset['motif_dataset'][motif]['test_labels']))\n",
    "            print('Training accuracy at epoch %d: %.1f%%' % (epoch, train_resuts[-1]))\n",
    "            print('Validation accuracy: %.1f%%' % valid_results[-1])\n",
    "            \n",
    "            #early stopping  #不能取到更好的结果时及时停止\n",
    "            if epoch > 10 and valid_results[-11] > max(valid_results[-10:]):\n",
    "                train_resuts = train_resuts[:-10]\n",
    "                valid_results = valid_results[:-10]\n",
    "                test_results = test_results[:-10]\n",
    "                motif_test_results = {motif: motif_test_results[motif][:-10] for motif in HUMAN_MOTIF_VARIANTS}\n",
    "                return train_resuts, valid_results, test_results, motif_test_results, save_weights[0]\n",
    "            \n",
    "            #model saving  #保存最新的参数\n",
    "            sw = {}\n",
    "            for k in weights:\n",
    "                sw[k] = weights[k].eval()\n",
    "            if epoch < 10:\n",
    "                save_weights.append(sw)\n",
    "            else:\n",
    "                save_weights.append(sw)\n",
    "                save_weights.pop(0)\n",
    "            \n",
    "    return train_resuts, valid_results, test_results, motif_test_results, save_weights[-1]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(_):\n",
    "    # block_print()\n",
    "\n",
    "    hyper_dict = gen_hyper_dict(HYPER_DICT)\n",
    "    pos_data, pos_labels, neg_data, neg_labels = produce_motif_dataset(NUM_FOLDS, POS_PATH, NEG_PATH)\n",
    "\n",
    "    # Cross validate\n",
    "    train_accuracy_split = []\n",
    "    valid_accuracy_split = []\n",
    "    test_accuracy_split = []\n",
    "    motif_test_accuracy_split = {motif: [] for motif in HUMAN_MOTIF_VARIANTS}\n",
    "    \n",
    "    for i in range(NUM_FOLDS):\n",
    "        split = {                                                            #交叉验证时split数据的方法\n",
    "            'train':[(i + j) % NUM_FOLDS for j in range(NUM_FOLDS-2)],\n",
    "            'valid':[(i + NUM_FOLDS - 2) % NUM_FOLDS],\n",
    "            'test': [(i + NUM_FOLDS - 1) % NUM_FOLDS]\n",
    "        }\n",
    "        save = motif_data_split(pos_data, pos_labels, neg_data, neg_labels, NUM_FOLDS, split)\n",
    "        dataset = {}\n",
    "        dataset['train_dataset'], dataset['train_labels'] = pad_dataset(save['train_dataset'], save['train_labels'])   #pad数据\n",
    "        dataset['valid_dataset'], dataset['valid_labels'] = pad_dataset(save['valid_dataset'], save['valid_labels'])\n",
    "        dataset['test_dataset'], dataset['test_labels'] = pad_dataset(save['test_dataset'], save['test_labels'])\n",
    "        dataset['motif_dataset'] = {}\n",
    "        for motif in HUMAN_MOTIF_VARIANTS:\n",
    "            dataset['motif_dataset'][motif] = {}\n",
    "            dataset['motif_dataset'][motif]['test_dataset'], dataset['motif_dataset'][motif]['test_labels'] = pad_dataset(save['motif_dataset'][motif]['test_dataset'], save['motif_dataset'][motif]['test_labels'])\n",
    "        train_resuts, valid_results, test_results, motif_test_results, save_weights = train(dataset, hyper_dict)\n",
    "        print(\"\\nbest valid epoch: %d\"%(len(train_resuts)-1))\n",
    "        print(\"Training accuracy: %.2f%%\"%train_resuts[-1])\n",
    "        print(\"Test accuracy: %.2f%%\"%test_results[-1])\n",
    "        print(\"Validation accuracy: %.2f%%\"%valid_results[-1])\n",
    "        for motif in HUMAN_MOTIF_VARIANTS:\n",
    "            print('*%s* accuracy: %.1f%%' % (motif, motif_test_results[motif][-1]))\n",
    "\n",
    "        #Dump model\n",
    "        if FLAGS.train_dir is not None:                                                   #保存weights到本地文件\n",
    "            with open(os.path.join(FLAGS.train_dir, 'cv%d_model.pkl'%i), \"wb\") as f:\n",
    "                pickle.dump(save_weights, f, 2)\n",
    "                \n",
    "        train_accuracy_split.append(train_resuts[-1])                               #保存预测的accuracy\n",
    "        valid_accuracy_split.append(valid_results[-1])\n",
    "        test_accuracy_split.append(test_results[-1])\n",
    "        for motif in HUMAN_MOTIF_VARIANTS:\n",
    "            motif_test_accuracy_split[motif].append(motif_test_results[motif][-1])\n",
    "        \n",
    "    train_accuracy = np.mean(train_accuracy_split)                         #取五次的均值作为最后的准确度\n",
    "    valid_accuracy = np.mean(valid_accuracy_split)\n",
    "    test_accuracy = np.mean(test_accuracy_split)\n",
    "    motif_test_accuracy = {}\n",
    "    for motif in HUMAN_MOTIF_VARIANTS:\n",
    "        motif_test_accuracy[motif] = np.mean(motif_test_accuracy_split[motif])\n",
    "    print('\\n\\n########################\\nFinal result:')\n",
    "    print('Training accuracy: %.1f%%' % (train_accuracy))\n",
    "    print('Validation accuracy: %.1f%%' % (valid_accuracy))\n",
    "    print('Test accuracy: %.1f%%' % (test_accuracy ))\n",
    "    for motif in HUMAN_MOTIF_VARIANTS:\n",
    "        print('*%s* accuracy: %.1f%%' % (motif, motif_test_accuracy[motif]))\n",
    "\n",
    "    if FLAGS.training_result_dir is not None:\n",
    "        with open(os.path.join(FLAGS.training_result_dir, 'result.pkl'), 'wb') as f:         #保存训练的参数到本地\n",
    "            hyper_dict['train_accuracy'] = train_accuracy\n",
    "            hyper_dict['valid_accuracy'] = valid_accuracy\n",
    "            hyper_dict['test_accuracy'] = test_accuracy\n",
    "            hyper_dict['motif_test_accuracy'] = motif_test_accuracy\n",
    "            pickle.dump(hyper_dict, f, 2)\n",
    "                   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive AATAAA: (2595, 206, 1, 4) (2595,)\n",
      "Negative AATAAA: (2595, 206, 1, 4) (2595,)\n",
      "Positive ATTAAA: (1200, 206, 1, 4) (1200,)\n",
      "Negative ATTAAA: (1200, 206, 1, 4) (1200,)\n",
      "Positive AAAAAG: (615, 206, 1, 4) (615,)\n",
      "Negative AAAAAG: (615, 206, 1, 4) (615,)\n",
      "Positive AAGAAA: (625, 206, 1, 4) (625,)\n",
      "Negative AAGAAA: (625, 206, 1, 4) (625,)\n",
      "Positive TATAAA: (390, 206, 1, 4) (390,)\n",
      "Negative TATAAA: (390, 206, 1, 4) (390,)\n",
      "Positive AATACA: (440, 206, 1, 4) (440,)\n",
      "Negative AATACA: (440, 206, 1, 4) (440,)\n",
      "Positive AGTAAA: (335, 206, 1, 4) (335,)\n",
      "Negative AGTAAA: (335, 206, 1, 4) (335,)\n",
      "Positive ACTAAA: (345, 206, 1, 4) (345,)\n",
      "Negative ACTAAA: (345, 206, 1, 4) (345,)\n",
      "Positive GATAAA: (230, 206, 1, 4) (230,)\n",
      "Negative GATAAA: (230, 206, 1, 4) (230,)\n",
      "Positive CATAAA: (205, 206, 1, 4) (205,)\n",
      "Negative CATAAA: (205, 206, 1, 4) (205,)\n",
      "Positive AATATA: (205, 206, 1, 4) (205,)\n",
      "Negative AATATA: (205, 206, 1, 4) (205,)\n",
      "Positive AATAGA: (185, 206, 1, 4) (185,)\n",
      "Negative AATAGA: (185, 206, 1, 4) (185,)\n",
      "WARNING:tensorflow:From <ipython-input-15-7d2634e2b429>:55: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Initialized\n",
      "Training accuracy at begining: 50.0%\n",
      "Validation accuracy at the beginning: 50.0%\n",
      "Training accuracy at epoch 0: 77.6%\n",
      "Validation accuracy: 77.1%\n",
      "Training accuracy at epoch 1: 86.8%\n",
      "Validation accuracy: 85.7%\n",
      "Training accuracy at epoch 2: 88.2%\n",
      "Validation accuracy: 86.4%\n",
      "Training accuracy at epoch 3: 90.8%\n",
      "Validation accuracy: 88.0%\n",
      "Training accuracy at epoch 4: 90.7%\n",
      "Validation accuracy: 87.6%\n",
      "Training accuracy at epoch 5: 91.7%\n",
      "Validation accuracy: 88.1%\n",
      "Training accuracy at epoch 6: 91.5%\n",
      "Validation accuracy: 88.7%\n",
      "Training accuracy at epoch 7: 93.4%\n",
      "Validation accuracy: 89.2%\n",
      "Training accuracy at epoch 8: 93.6%\n",
      "Validation accuracy: 89.6%\n",
      "Training accuracy at epoch 9: 91.6%\n",
      "Validation accuracy: 87.7%\n",
      "Training accuracy at epoch 10: 91.5%\n",
      "Validation accuracy: 85.6%\n",
      "Training accuracy at epoch 11: 94.1%\n",
      "Validation accuracy: 88.3%\n",
      "Training accuracy at epoch 12: 93.6%\n",
      "Validation accuracy: 88.3%\n",
      "Training accuracy at epoch 13: 93.4%\n",
      "Validation accuracy: 88.2%\n",
      "Training accuracy at epoch 14: 94.2%\n",
      "Validation accuracy: 88.8%\n",
      "Training accuracy at epoch 15: 92.7%\n",
      "Validation accuracy: 87.8%\n",
      "Training accuracy at epoch 16: 93.2%\n",
      "Validation accuracy: 88.3%\n",
      "Training accuracy at epoch 17: 94.3%\n",
      "Validation accuracy: 88.4%\n",
      "Training accuracy at epoch 18: 94.6%\n",
      "Validation accuracy: 88.1%\n",
      "\n",
      "best valid epoch: 8\n",
      "Training accuracy: 93.57%\n",
      "Test accuracy: 89.45%\n",
      "Validation accuracy: 89.59%\n",
      "*AATAAA* accuracy: 87.9%\n",
      "*ATTAAA* accuracy: 91.9%\n",
      "*AAAAAG* accuracy: 94.3%\n",
      "*AAGAAA* accuracy: 90.8%\n",
      "*TATAAA* accuracy: 87.8%\n",
      "*AATACA* accuracy: 89.2%\n",
      "*AGTAAA* accuracy: 87.3%\n",
      "*ACTAAA* accuracy: 85.5%\n",
      "*GATAAA* accuracy: 89.1%\n",
      "*CATAAA* accuracy: 85.4%\n",
      "*AATATA* accuracy: 89.0%\n",
      "*AATAGA* accuracy: 95.9%\n",
      "Initialized\n",
      "Training accuracy at begining: 50.0%\n",
      "Validation accuracy at the beginning: 50.0%\n",
      "Training accuracy at epoch 0: 79.5%\n",
      "Validation accuracy: 78.6%\n",
      "Training accuracy at epoch 1: 72.1%\n",
      "Validation accuracy: 72.6%\n",
      "Training accuracy at epoch 2: 89.3%\n",
      "Validation accuracy: 87.2%\n",
      "Training accuracy at epoch 3: 91.4%\n",
      "Validation accuracy: 88.3%\n",
      "Training accuracy at epoch 4: 91.8%\n",
      "Validation accuracy: 89.1%\n",
      "Training accuracy at epoch 5: 92.0%\n",
      "Validation accuracy: 89.0%\n",
      "Training accuracy at epoch 6: 90.0%\n",
      "Validation accuracy: 86.7%\n",
      "Training accuracy at epoch 7: 77.4%\n",
      "Validation accuracy: 77.1%\n",
      "Training accuracy at epoch 8: 91.6%\n",
      "Validation accuracy: 88.8%\n",
      "Training accuracy at epoch 9: 91.9%\n",
      "Validation accuracy: 88.9%\n",
      "Training accuracy at epoch 10: 92.4%\n",
      "Validation accuracy: 88.6%\n",
      "Training accuracy at epoch 11: 91.0%\n",
      "Validation accuracy: 88.5%\n",
      "Training accuracy at epoch 12: 91.7%\n",
      "Validation accuracy: 88.2%\n",
      "Training accuracy at epoch 13: 91.6%\n",
      "Validation accuracy: 88.1%\n",
      "Training accuracy at epoch 14: 93.1%\n",
      "Validation accuracy: 88.8%\n",
      "\n",
      "best valid epoch: 4\n",
      "Training accuracy: 91.79%\n",
      "Test accuracy: 88.91%\n",
      "Validation accuracy: 89.08%\n",
      "*AATAAA* accuracy: 87.2%\n",
      "*ATTAAA* accuracy: 90.4%\n",
      "*AAAAAG* accuracy: 95.1%\n",
      "*AAGAAA* accuracy: 87.6%\n",
      "*TATAAA* accuracy: 91.0%\n",
      "*AATACA* accuracy: 88.1%\n",
      "*AGTAAA* accuracy: 89.6%\n",
      "*ACTAAA* accuracy: 87.0%\n",
      "*GATAAA* accuracy: 87.0%\n",
      "*CATAAA* accuracy: 87.8%\n",
      "*AATATA* accuracy: 89.0%\n",
      "*AATAGA* accuracy: 90.5%\n",
      "Initialized\n",
      "Training accuracy at begining: 46.2%\n",
      "Validation accuracy at the beginning: 45.6%\n",
      "Training accuracy at epoch 0: 78.8%\n",
      "Validation accuracy: 77.1%\n",
      "Training accuracy at epoch 1: 84.0%\n",
      "Validation accuracy: 82.7%\n",
      "Training accuracy at epoch 2: 84.3%\n",
      "Validation accuracy: 82.6%\n",
      "Training accuracy at epoch 3: 89.9%\n",
      "Validation accuracy: 87.7%\n",
      "Training accuracy at epoch 4: 90.4%\n",
      "Validation accuracy: 87.3%\n",
      "Training accuracy at epoch 5: 91.7%\n",
      "Validation accuracy: 88.1%\n",
      "Training accuracy at epoch 6: 90.3%\n",
      "Validation accuracy: 87.5%\n",
      "Training accuracy at epoch 7: 88.9%\n",
      "Validation accuracy: 86.4%\n",
      "Training accuracy at epoch 8: 86.0%\n",
      "Validation accuracy: 84.6%\n",
      "Training accuracy at epoch 9: 93.0%\n",
      "Validation accuracy: 90.0%\n",
      "Training accuracy at epoch 10: 92.8%\n",
      "Validation accuracy: 89.5%\n",
      "Training accuracy at epoch 11: 92.7%\n",
      "Validation accuracy: 89.5%\n",
      "Training accuracy at epoch 12: 92.6%\n",
      "Validation accuracy: 88.8%\n",
      "Training accuracy at epoch 13: 93.6%\n",
      "Validation accuracy: 89.5%\n",
      "Training accuracy at epoch 14: 94.2%\n",
      "Validation accuracy: 89.6%\n",
      "Training accuracy at epoch 15: 94.2%\n",
      "Validation accuracy: 89.5%\n",
      "Training accuracy at epoch 16: 93.0%\n",
      "Validation accuracy: 88.7%\n",
      "Training accuracy at epoch 17: 93.8%\n",
      "Validation accuracy: 88.8%\n",
      "Training accuracy at epoch 18: 90.9%\n",
      "Validation accuracy: 86.4%\n",
      "Training accuracy at epoch 19: 90.1%\n",
      "Validation accuracy: 85.3%\n",
      "\n",
      "best valid epoch: 9\n",
      "Training accuracy: 93.03%\n",
      "Test accuracy: 89.31%\n",
      "Validation accuracy: 90.03%\n",
      "*AATAAA* accuracy: 87.0%\n",
      "*ATTAAA* accuracy: 91.2%\n",
      "*AAAAAG* accuracy: 92.7%\n",
      "*AAGAAA* accuracy: 93.6%\n",
      "*TATAAA* accuracy: 88.5%\n",
      "*AATACA* accuracy: 85.2%\n",
      "*AGTAAA* accuracy: 89.6%\n",
      "*ACTAAA* accuracy: 87.0%\n",
      "*GATAAA* accuracy: 90.2%\n",
      "*CATAAA* accuracy: 90.2%\n",
      "*AATATA* accuracy: 90.2%\n",
      "*AATAGA* accuracy: 95.9%\n",
      "Initialized\n",
      "Training accuracy at begining: 50.0%\n",
      "Validation accuracy at the beginning: 50.0%\n",
      "Training accuracy at epoch 0: 79.2%\n",
      "Validation accuracy: 79.8%\n",
      "Training accuracy at epoch 1: 85.6%\n",
      "Validation accuracy: 85.5%\n",
      "Training accuracy at epoch 2: 87.9%\n",
      "Validation accuracy: 86.7%\n",
      "Training accuracy at epoch 3: 88.5%\n",
      "Validation accuracy: 87.1%\n",
      "Training accuracy at epoch 4: 87.7%\n",
      "Validation accuracy: 86.3%\n",
      "Training accuracy at epoch 5: 90.5%\n",
      "Validation accuracy: 88.7%\n",
      "Training accuracy at epoch 6: 87.0%\n",
      "Validation accuracy: 85.0%\n",
      "Training accuracy at epoch 7: 90.2%\n",
      "Validation accuracy: 87.3%\n",
      "Training accuracy at epoch 8: 90.9%\n",
      "Validation accuracy: 86.8%\n",
      "Training accuracy at epoch 9: 87.4%\n",
      "Validation accuracy: 84.9%\n",
      "Training accuracy at epoch 10: 92.5%\n",
      "Validation accuracy: 88.8%\n",
      "Training accuracy at epoch 11: 91.0%\n",
      "Validation accuracy: 86.3%\n",
      "Training accuracy at epoch 12: 90.9%\n",
      "Validation accuracy: 86.8%\n",
      "Training accuracy at epoch 13: 93.6%\n",
      "Validation accuracy: 88.6%\n",
      "Training accuracy at epoch 14: 93.3%\n",
      "Validation accuracy: 87.2%\n",
      "Training accuracy at epoch 15: 93.2%\n",
      "Validation accuracy: 86.2%\n",
      "Training accuracy at epoch 16: 92.5%\n",
      "Validation accuracy: 87.0%\n",
      "Training accuracy at epoch 17: 90.9%\n",
      "Validation accuracy: 86.4%\n",
      "Training accuracy at epoch 18: 91.8%\n",
      "Validation accuracy: 85.8%\n",
      "Training accuracy at epoch 19: 92.7%\n",
      "Validation accuracy: 86.2%\n",
      "Training accuracy at epoch 20: 90.3%\n",
      "Validation accuracy: 86.0%\n",
      "\n",
      "best valid epoch: 10\n",
      "Training accuracy: 92.55%\n",
      "Test accuracy: 88.98%\n",
      "Validation accuracy: 88.77%\n",
      "*AATAAA* accuracy: 86.6%\n",
      "*ATTAAA* accuracy: 89.8%\n",
      "*AAAAAG* accuracy: 93.1%\n",
      "*AAGAAA* accuracy: 94.4%\n",
      "*TATAAA* accuracy: 84.6%\n",
      "*AATACA* accuracy: 86.4%\n",
      "*AGTAAA* accuracy: 91.8%\n",
      "*ACTAAA* accuracy: 92.0%\n",
      "*GATAAA* accuracy: 90.2%\n",
      "*CATAAA* accuracy: 85.4%\n",
      "*AATATA* accuracy: 84.1%\n",
      "*AATAGA* accuracy: 97.3%\n",
      "Initialized\n",
      "Training accuracy at begining: 50.9%\n",
      "Validation accuracy at the beginning: 51.7%\n",
      "Training accuracy at epoch 0: 77.0%\n",
      "Validation accuracy: 76.6%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy at epoch 1: 82.1%\n",
      "Validation accuracy: 81.7%\n",
      "Training accuracy at epoch 2: 85.7%\n",
      "Validation accuracy: 84.1%\n",
      "Training accuracy at epoch 3: 88.8%\n",
      "Validation accuracy: 87.3%\n",
      "Training accuracy at epoch 4: 90.7%\n",
      "Validation accuracy: 88.3%\n",
      "Training accuracy at epoch 5: 90.6%\n",
      "Validation accuracy: 88.7%\n",
      "Training accuracy at epoch 6: 90.2%\n",
      "Validation accuracy: 87.8%\n",
      "Training accuracy at epoch 7: 91.0%\n",
      "Validation accuracy: 88.1%\n",
      "Training accuracy at epoch 8: 93.0%\n",
      "Validation accuracy: 89.5%\n",
      "Training accuracy at epoch 9: 93.4%\n",
      "Validation accuracy: 89.5%\n",
      "Training accuracy at epoch 10: 90.9%\n",
      "Validation accuracy: 87.4%\n",
      "Training accuracy at epoch 11: 92.6%\n",
      "Validation accuracy: 89.0%\n",
      "Training accuracy at epoch 12: 93.7%\n",
      "Validation accuracy: 89.1%\n",
      "Training accuracy at epoch 13: 93.2%\n",
      "Validation accuracy: 88.3%\n",
      "Training accuracy at epoch 14: 93.0%\n",
      "Validation accuracy: 88.4%\n",
      "Training accuracy at epoch 15: 92.9%\n",
      "Validation accuracy: 88.2%\n",
      "Training accuracy at epoch 16: 93.2%\n",
      "Validation accuracy: 88.0%\n",
      "Training accuracy at epoch 17: 92.8%\n",
      "Validation accuracy: 88.5%\n",
      "Training accuracy at epoch 18: 92.7%\n",
      "Validation accuracy: 87.9%\n",
      "\n",
      "best valid epoch: 8\n",
      "Training accuracy: 92.96%\n",
      "Test accuracy: 89.99%\n",
      "Validation accuracy: 89.52%\n",
      "*AATAAA* accuracy: 88.2%\n",
      "*ATTAAA* accuracy: 90.0%\n",
      "*AAAAAG* accuracy: 98.4%\n",
      "*AAGAAA* accuracy: 90.4%\n",
      "*TATAAA* accuracy: 91.7%\n",
      "*AATACA* accuracy: 87.5%\n",
      "*AGTAAA* accuracy: 90.3%\n",
      "*ACTAAA* accuracy: 87.7%\n",
      "*GATAAA* accuracy: 89.1%\n",
      "*CATAAA* accuracy: 90.2%\n",
      "*AATATA* accuracy: 89.0%\n",
      "*AATAGA* accuracy: 93.2%\n",
      "\n",
      "\n",
      "########################\n",
      "Final result:\n",
      "Training accuracy: 92.8%\n",
      "Validation accuracy: 89.4%\n",
      "Test accuracy: 89.3%\n",
      "*AATAAA* accuracy: 87.4%\n",
      "*ATTAAA* accuracy: 90.7%\n",
      "*AAAAAG* accuracy: 94.7%\n",
      "*AAGAAA* accuracy: 91.4%\n",
      "*TATAAA* accuracy: 88.7%\n",
      "*AATACA* accuracy: 87.3%\n",
      "*AGTAAA* accuracy: 89.7%\n",
      "*ACTAAA* accuracy: 87.8%\n",
      "*GATAAA* accuracy: 89.1%\n",
      "*CATAAA* accuracy: 87.8%\n",
      "*AATATA* accuracy: 88.3%\n",
      "*AATAGA* accuracy: 94.6%\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2971: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
